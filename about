The Sonic Alchemy of MetaSynth and the Digital Sculpting of Composers Desktop Project

The late 90s and early 2000s were a fertile ground for experimental electronic music, with artists pushing the boundaries of sound design. Two software tools that played a significant, albeit different, role in this exploration were U&I Software's MetaSynth and the Composers Desktop Project (CDP). While Aphex Twin's seminal "Windowlicker" EP is famously associated with MetaSynth's unique capabilities, understanding both pieces of software reveals a fascinating dichotomy in approaching sound manipulation.
MetaSynth: Painting with Sound

At its core, MetaSynth is a digital audio workstation that offers a revolutionary approach to synthesis: converting images directly into sound. This is the feature that captured the imagination of artists like Richard D. James (Aphex Twin) and was reportedly used in creating the iconic "bullet time" sound effects in the movie The Matrix.[1][2]
Visually Explained: The Image Synth

Imagine a canvas, much like a simple paint program. This is the central workspace of MetaSynth's Image Synth room.[1][3] Here's how it visually translates to sound:

    The Horizontal Axis (X-axis): Time. As a playhead scans across the image from left to right, the visual information is converted into audio. The further to the right a pixel is, the later it will sound.[4]

    The Vertical Axis (Y-axis): Pitch. The vertical position of a pixel determines its pitch. Higher pixels correspond to higher frequencies, and lower pixels to lower frequencies.[4] You can define the musical scale, allowing for everything from traditional chromatic scales to microtonal explorations.

    Brightness and Color: Amplitude and Pan. The brightness of a pixel dictates its volume or amplitude – the brighter the pixel, the louder the sound.[4] Color introduces a stereo dimension: red components are typically panned to the left channel, green to the right, and yellow (a combination of red and green) to the center.[5][6] Blue is often used for a non-audible grid or for special functions.[6]

This direct and intuitive visual-to-auditory mapping allows for unprecedented control over sound design. One could literally "paint" a melody, a chord, or a complex texture. The snarling, metallic, and ghostly vocal manipulations in "Windowlicker" are prime examples of what happens when organic forms, like a face, are translated into sound via this method. It is widely reported that Aphex Twin used a picture of his own face to generate some of the iconic sounds on the EP's B-side, "Formula".[1]

MetaSynth is not limited to just the Image Synth. It's a comprehensive sound design environment with six main "rooms," including an effects room with drawable envelopes, a spectrum synth for granular synthesis, an image filter that uses pictures to create dynamic filters, a sequencer, and a multitrack montage room to assemble compositions.[1][7]
Composers Desktop Project (CDP): The Sound Sculptor's Toolkit

In contrast to MetaSynth's visual immediacy, the Composers Desktop Project (CDP) is a collection of powerful, non-real-time command-line tools for in-depth sound manipulation.[8][9] It's less about creating sound from a visual source and more about the intricate transformation of existing audio. CDP is a suite of hundreds of processes that allow for deep spectral editing, morphing, and algorithmic composition.[2][10]
Visually Explained: A Text-Based Powerhouse with GUI Front-Ends

Originally, CDP was entirely operated via a command-line interface, a text-based environment where users would type commands to process audio files. This offered immense power and precision but lacked the immediate visual feedback of MetaSynth.

To make it more accessible, graphical user interfaces (GUIs) like SoundLoom and Soundshaper were developed.[11] These interfaces provide a more visual way to interact with CDP's processes:

    Soundshaper: This Windows-based GUI presents a grid-like "patch bay" where you can chain different processes together.[11] You would load a sound into a cell, select a process from a menu (e.g., spectral filtering, time-stretching), adjust its parameters in a dedicated window, and then render the output to a new cell.[1][11] This creates a visual signal flow from left to right.

    SoundLoom: This cross-platform GUI, created by CDP developer Trevor Wishart, offers a workspace where you can gather and choose sound files for processing.[11][12] Like Soundshaper, it provides menus and parameter boxes to interact with the powerful underlying CDP processes.

The visual experience of CDP is therefore more akin to a laboratory or a workshop than an artist's canvas. The focus is on the precise manipulation of sonic data through a vast array of functions covering spectral analysis, formant manipulation, granular synthesis, and more.[2]
Unique Features for a Modern VST

If a developer were to create a modern VST plugin inspired by the groundbreaking features of MetaSynth and the deep processing of CDP, it could offer a truly unique and powerful tool for contemporary music producers.
The "Spectra-Canvas" VST: A Visual Concept

Imagine a VST with a central, high-resolution, multi-layered canvas. Here’s how its unique features, inspired by MetaSynth, could be implemented:

    Layer-Based Image Synthesis: The canvas wouldn't be a single flat image but would support multiple layers, similar to modern image editing software. Each layer could have its own independent sound source (a built-in oscillator, a sampler, or even another audio track from the DAW). This would allow for the creation of incredibly complex and evolving soundscapes by blending different "painted" sonic elements.

    Dynamic and Modulatable Brushes: Instead of static drawing tools, the VST would feature "brushes" with assignable modulation sources. For example, a brush's size could be linked to an LFO, its color to velocity, and its opacity to aftertouch. This would allow for expressive, real-time "performance" of sonic textures directly onto the canvas. A "rhythm brush" could paint a sequence of pixels based on a MIDI clip or a predefined rhythmic pattern.

    Vector-Based Drawing and Warping: Rather than being limited to a pixel grid, users could draw resolution-independent vector shapes. These shapes could then be warped and animated over time, with the VST recalculating the resulting audio in real time. Imagine drawing a smooth melodic contour and then applying a "turbulence" modulator to create subtle pitch variations.

    Integrated Spectral and Waveform Views: Seamlessly integrated into the canvas interface would be a real-time spectral and waveform display of the generated audio. Users could directly manipulate the spectrum – for example, by "erasing" certain frequencies or "smudging" harmonics – and see the corresponding visual changes on the canvas and hear the immediate sonic result. This would bridge the gap between MetaSynth's image-based approach and CDP's spectral manipulation.

    AI-Powered Image Generation: A truly modern twist would be the integration of AI image generation. Users could input text prompts (e.g., "a shimmering glass cityscape at dawn") and the VST would generate a unique visual texture to be used as a sound source, offering an endless wellspring of sonic inspiration.

By combining the intuitive, visual sound-painting of MetaSynth with the deep, surgical precision of spectral manipulation and modern VST features, such a plugin would not only pay homage to these pioneering tools but also open up entirely new realms of sonic possibility for the next generation of music producers.
Sources help 
